# 第一篇：Vision Transformer (ViT)
1. 本文主要关注纯 **Transformer** 直接应用于图像块序列，因为在视觉领域，注意力机制要么与卷积网络结合使用，要么用于替换卷积网络的某些组件，从来都没有用纯Transformer来处理图像的，而且用传统方法需要经过**大规模的训练**，所以作者想试试有没有其他的方法，不想依赖卷积架构
2. 在CV领域，传统方式都是**卷积架构**，即类 ResNet 架构，由于卷积架构的局部性和自注意力的特征，每个像素都需要关注其他所有像素，导致需要经过大规模的训练，而且如果transformer如果真的能够应用，**可扩展**的 NLP Transformer 架构及其高效几乎可以直接使用，这可以规避卷积架构的一些弊端，比如说与卷积神经网络相比，视觉 Transformer 具有更少的图像特定**归纳偏置**
3. 作者主要进行了以下几步
   1. 图像 patch 化与嵌入
   2. 分类 token 设计
   3. 位置嵌入设计
   4. 复用 NLP 架构
   5. 预训练与微调
   6. 混合架构
4. 为了证明自己的方法是有效的作者进行了：
   1. 与文献中最先进的卷积神经网络进行**比较**，如Big Transfer和Noisy Student
   2. **不同规模数据集**上预训练 ViT，发现在大数据集中，ViT-Large、ViT-Huge 性能显著超越 ViT-Base，ViT 在极低数据量迁移场景中具有良好的潜力
   3. 对不同模型进行了对照**扩展性**研究：在这种情况下，数据规模不会限制模型性能，因此可以评估每个模型的性能与预训练成本之间的关系，最终发现视觉 Transformer 的性能竟尚未达到饱和
   4. 视觉 Transformer 的深入分析，对其内部进行深入分析
5. 我觉得这篇文章是具有里程碑式的意义，他打破了传统的基于卷积架构的模型，而是创新性的使用纯 **Transformer**来进行图形处理，并且在效果验证上，与最先进的模型对比，验证了自己的想法而又不失严谨，最后又自己分析了视觉 Transformer内部来从原理上解释

## 思考
传统的卷积架构，虽然在小数据处理上又优势，但是一到大数据，训练量就呈指数级增长，所以transformer就解决了这一点，在大数据上与训练后，仅需微调就可以直接用于小数据集的处理

# 第二篇：CLIP
1. 本文主要是讲了**CLIP**的 AI 模型，它通过 “看图像 + 读文字” 的组合来学习，因为传统的方法要提前进行标签的标注，对于训练之外的图像适用性差，必须额外标注并重新训练，本文旨在解决这个问题
2. 之前的研究用到过很多方法，比如说训练卷积神经网络（CNN）预测图像描述中的单词，能学习到有效的图像表示，基于 Transformer 的语言建模、掩码语言建模和对比目标函数，可从文本中学习图像表示，但是都需要提前**定义固定类别**，标签是固定的，无法处理 “开放集” 视觉概念
3. 本文主要是通过 “自然语言监督 + 对比学习”，让模型学会自己分类，并支持零样本迁移到任意视觉任务，无需任务特定训练数据，他的步骤主要有：
   1. 构建足够大的**数据集**
   2. 选择高效的**预训练**方法，将任务简化为 “仅预测哪段文本与哪张图像整体配对，而非预测文本的精确单词”
   3. 模型选择与规模**扩展**：ResNet 系列和视觉 Transformer（ViT）
4. 验证有效性的方法：
   1. **零样本迁移**实验：验证任务适配能力（Visual N-Grams）：CLIP 在所有三个数据集上均大幅提升性能
   2. **表示学习**实验：验证特征通用性：在线性分类器拟合和端到端微调两个方法中选择前者，发现CLIP 模型学习的任务**范围远超** “从随机初始化端到端训练的单一计算机视觉模型” 的传统能力
   3. **鲁棒性**实验：验证分布偏移适应力：CLIP 模型学习的任务范围远超 “从随机初始化端到端训练的单一计算机视觉模型” 的传统能力。之后又对自然偏移下的鲁棒性进行了分析，结果仍理想
   4. **与人类性能的对比**
   5. **数据重叠分析**： CLIP 的性能提升并非源于数据泄露。
5. 这篇文章是具有里程碑式的意义，首次将 “**自然语言监督**” 应用于视觉模型预训练，打破了传统模型的 “固定类别依赖”，实现了通过文本定义任务，拓展了学习能力。而且为了验证有效性，作者十分严谨，如ImageNet 模型与 CLIP 的鲁棒性对比，适配 ImageNet 分布对鲁棒性的影响，从无监督到全监督的鲁棒性变化从多个角度验证其有效性。
   但是也有**不足**的地方，比如说对数据稀缺场景不友好，训练的东西太少；特定任务性能薄弱：无法像图像描述模型那样生成新颖输出；极端分布外数据的泛化能力差；数据效率低等等

## 思考
前人工作优劣：
- 单任务（如 ImageNet 分类）性能强，工程成熟；
- 依赖固定人工标注类别，泛化差，无法零样本迁移。

# 第三篇：BLIP
1. 本文主要关注了视觉-语言预训练的两大局限性：
   1. **模型层面**：基于编码器的模型难以直接迁移至文本生成类任务（如图像描述生成），而编码器 - 解码器模型也未能成功应用于图像 - 文本检索任务。
   2. **数据层面**：网络文本中的噪声对视觉 - 语言学习而言并非最优选择。
2. 之前的视觉-语言预训练，一类是基于**编码器**的模型，另一类是**编码器-解码器**模型，均采用从网络爬取的图像与替代文本对，并用了简单的规则过滤，但网络文本中的噪声仍普遍存在。而且无法既处理理解类任务，又处理生成类任务
3. 作者的做法：
   1. **模型架构**：多模态编码器 - 解码器混合结构：包含单模态编码器，图像引导文本编码器，图像引导文本解码器
   2. **预训练**目标函数：通过训练三种损失函数：图像-文本对比损失，图像 - 文本匹配损失，语言建模损失
   3. **数据处理**：通过“描述生成器” 和 “过滤器”，很大程度上减小噪声，使得数据的质量得以提升，模型的能力得以提升。
4. 作者的验证方法：
   1. 对比了在**不同数据集**上预训练的模型性能，以验证 CapFilt 在下游任务中的有效性，发现性能提升，相比使用原始噪声网络文本，性能得到显著提升
   2. 评估**共享参数**：在 1400 万幅网络图像文本上预训练、采用不同参数共享策略的模型性能，发现共享除自注意力层外的所有参数不仅能提升性能，还能减小模型规模。
   3. 与当前最优方法**对比**：将 BLIP 与现有视觉 - 语言预训练方法在多个视觉语言任务中进行对比，发现BLIP的性能更好
   4. 此外，作者还针对 CapFilt 方法进行额外的**消融实验**，证明了CapFilt 的性能提升并非源于训练时长增加
5. 本文作者非常创新，利用 MED 架构实现了既处理理解类任务，又处理生成类任务，同时又利用了CapFilt实现了对噪声数据的过滤，具有里程碑式的意义，而且作者为了严谨，又针对 CapFilt 方法进行额外的消融实验。
   但是也有需要改进的地方，比如说为每幅图像生成多个合成描述，进一步扩大预训练语料库等等

## 思考
- 前人通过爬取大规模网络图像 - 文本对扩大预训练数据规模，通过数据量的提升显著改善 VLP 模型性能，但是编码器模型无法直接支持文本生成，编码器 - 解码器模型在检索任务中效率低、性能差，无法既处理理解类任务，又处理生成类任务。
- 在读了这篇文章后我就在思考，CLIP和BLIP到底有什么区别，我觉得 BLIP 在 CLIP 基础上进行了改进，引入字幕生成和过滤模块，去除噪声数据，使得模型的质量更高，而且 CLIP 适用于在零样本学习，但是 BLIP 还可以用于文本生成

# 第四篇：BLIP-2
1. 本文主要关注由于大规模模型的**端到端训练**，导致视觉 - 语言预训练的**成本**已变得越来越高昂这一问题，因为无论是CLIP还是其他模型，都是为了减少成本从而努力解决大数据成本这一问题，而且现有的冻结预训练图像编码器和冻结大型语言模型，这些模型都很高级，但是他们之间没法产生联系，而且前人对这方面的研究还不够
2. 前人的主要做法：要么是端到端训练，但是会使成本高昂，也有尝试融合冻结预训练图像编码器和冻结大型语言模型的，但是效果不理想，还是存在模态差距
3. 作者则是提出了**BLIP-2**这一模型，通过轻量级查询转换器Q-Former建立了图像编码器和语言模型之间的联系，具体是分了两个阶段：
   - 结合冻结图像编码器的视觉 - 语言表征学习阶段：将Q-Former与冻结图像编码器相连，使其能够提取这个图片中的关键视觉特征，并优化三个预训练目标，实现让Q-Former从 “会对比” 到 “会生成” 再到 “会判断”
   - 结合冻结大型语言模型的视觉到语言生成式学习阶段：作者使用了两种类型的大型语言模型
 - 在其中起到连接作用的Q-Former是最重要的，它包含两个**共享自注意力层的** Transformer 子模块。
 - 在**预训练**上，采用宇BLIP相同的数据集，并用CapFilt方法生成并筛选高质量图像描述
4. 实验验证：
   1. 零样本图像到文本生成
   2. 图像描述生成：
   3. 视觉问答
   4. 图像 - 文本检索
5. 但是BLIP-2也存在一定的局限性，比如说缺乏上下文学习能力，并且由于使用了冻结模型，它也直接继承了缺点，如输出冒犯性语言、传播社会偏见或泄露隐私信息等等，今后可以在引导方面和过滤有害数据集方面继续努力
   尽管如此，BLIP-2还是在视觉 - 语言任务上实现了重大突破，而且由于是直接使用的冻结语言模型，这代表它可以使用更先进的语言模型，可以直接套用语言模型方面的拓展性

## 思考：BLIP与BLIP-2的关系：
- BLIP-2 是 BLIP 的改进版本，继承了 BLIP 的部分技术思路，比如说使用了 CapFilt ，还用了共享自注意力层的 Transformer
- BLIP想构建一个统一体，而BLIP-2主要是利用现有的单模型，比如预训练图像编码器和大型语言模型，而且BLIP在优化时参数会变化，而BLIP-2由于使用的是冻结模型，因此在训练时参数权重不变。而且我觉得BLIP-2与BLIP相比，还具备了文本生成能力
